{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meguazy/project_CSD/blob/main/notebook_exploration_cleaning/tsne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sDU0SpvewTeN",
        "outputId": "f43c1fa1-e005-439c-9c53-bb2349a37afd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'project_CSD'...\n",
            "remote: Enumerating objects: 434, done.\u001b[K\n",
            "remote: Counting objects: 100% (322/322), done.\u001b[K\n",
            "remote: Compressing objects: 100% (268/268), done.\u001b[K\n",
            "remote: Total 434 (delta 81), reused 220 (delta 35), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (434/434), 13.50 MiB | 19.53 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/Meguazy/project_CSD.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd project_CSD/"
      ],
      "metadata": {
        "id": "6mdYIoh0tI9k",
        "outputId": "8be4c95d-ef28-4e0f-c959-50329a33bc6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/project_CSD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Usare ogni volta che si inizia a lavorare per accertarsi che non ci siano\n",
        "#cambiamenti non sincronizzati\n",
        "\n",
        "!git pull"
      ],
      "metadata": {
        "id": "qRRxymcztAiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import requests\n",
        "gcloud_token = !gcloud auth print-access-token\n",
        "gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "EMAIL = str(gcloud_tokeninfo['email'])\n",
        "\n",
        "!echo $EMAIL\n",
        "\n",
        "#Usare per fare commit atomici e frequenti.\n",
        "#Ricordiamoci di usare mettere sempre dei messaggi di commit chiari in modo da\n",
        "#poter rollbackare o cherry-pickare in caso di bisogno.\n",
        "\n",
        "!git config --global user.email $EMAIL\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Added first pre-processing logic\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "rTwhriuWu7WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CALCOLO L'ENTROPIA TOTALE DEL DATASET DI PARTENZA\n",
        "\n",
        "from scipy.stats import entropy\n",
        "\n",
        "def calculate_entropy(X_scaled):\n",
        "    # Setto una costante in modo da avere il meno possibile probabilità uguali a 0.\n",
        "    # Questo perché scipy.stats.entropy assume che il vettore passato non abbia\n",
        "    # probabilità uguali a 0.\n",
        "    epsilon = 1e-10\n",
        "\n",
        "    # Modifico il dataset \"X_scaled\" (quello iniziale) aggiungendo epsilon\n",
        "    # Qui eseguo anche il valore assoluto perché chiaramente le probabilità non sono\n",
        "    # definite per valori negativi.\n",
        "    joint_probs = np.abs(X_scaled.flatten()) + epsilon\n",
        "\n",
        "    # In alcuni casi, per valori troppo vicini allo zero, python setta\n",
        "    # automaticamente a 0 la variabile anche dopo aver aggiunto epsilon.\n",
        "    # Dato che le probabilità a zero non hanno alcun peso sull'entropia di un\n",
        "    # oggetto, semplicemente le filtro.\n",
        "    nonzero_probs = joint_probs[joint_probs > 0]\n",
        "\n",
        "    if len(nonzero_probs) == 0:\n",
        "        print(\"Tutte le probabilità sono zero. Impossibile calcolare l'entropia\")\n",
        "        joint_entropy = 1\n",
        "    else:\n",
        "        # base=2 significa che sto usando logaritmi in base 2 per calcolare l'entropia\n",
        "        # Possono essere usati anche altri logaritmi, ma la base 2 è quella usata dal\n",
        "        # t-sne. Ergo, dobbiamo restare sulla stessa base di logaritmo anche per il totale.\n",
        "        joint_entropy = entropy(nonzero_probs, base=2)\n",
        "    return joint_entropy"
      ],
      "metadata": {
        "id": "GRNBKDGuiVVt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# APPLICO IL T-SNE E CALCOLO LA KL-DIVERGENCE\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "mean_perc_array = np.zeros(2)\n",
        "\n",
        "for traino_number in range(1,3):\n",
        "    dir_path = \"data/processed_data/TrainoCaso\" + str(traino_number) + \"/\"\n",
        "    raw_dir_path = \"data/raw_data/DatasetRuoteCaso\" + str(traino_number) + \"/\"\n",
        "\n",
        "    time_series_df = pd.read_csv(dir_path + \"time_series.csv\")\n",
        "\n",
        "    num_files = len([f for f in os.listdir(raw_dir_path)if os.path.isfile(os.path.join(raw_dir_path, f))])\n",
        "\n",
        "    final_df = pd.DataFrame()\n",
        "\n",
        "    perc_sum = 0\n",
        "\n",
        "    for acq_num in range(1, num_files + 1):\n",
        "        # Seleziono, in questo caso, l'acquisizione numero acq_num\n",
        "        selected_df = time_series_df.loc[time_series_df['Acquisition Number'] == acq_num]\n",
        "\n",
        "        # Facendo così prendo solo la prima time series dell'acquisizione.\n",
        "        df_axe1 = selected_df[['Axe1X', 'Axe1Y', 'Axe1Z']]\n",
        "        df_axe2 = selected_df[['Axe2X', 'Axe2Y', 'Axe2Z']]\n",
        "\n",
        "        # Se avessi voluto prendere la seconda (perché ricordiamo che abbiamo 2 time series ad acquisizione)\n",
        "        # avrei dovuto scrivere la seguente linea di codice:\n",
        "        # selected_df = time_series_df[['Axe2X', 'Axe2Y', 'Axe2Z']]\n",
        "        # Da tenere a mente che il dataset finale dovrà contenere ENTRAMBE le time series sullo stesso record.\n",
        "        # Quindi, la struttura finale della tabella dovrà essere:\n",
        "        # [time, time_series_1, time_series_2]\n",
        "        # Mentre quella attuale è\n",
        "        # [time, 'Axe1X', 'Axe1Y', 'Axe1Z', 'Axe2X', 'Axe2Y', 'Axe2Z']\n",
        "        # Per schematizzare ancora meglio:\n",
        "        # 1) 'Axe1X', 'Axe1Y', 'Axe1Z' ---> (tramite il t-sne) time_series_1\n",
        "        # 2) 'Axe2X', 'Axe2Y', 'Axe2Z' ---> (tramite il t-sne) time_series_2\n",
        "\n",
        "        # Normalizzo i dati\n",
        "        sc = StandardScaler()\n",
        "        X_scaled_axe1 = sc.fit_transform(df_axe1)\n",
        "        X_scaled_axe2 = sc.fit_transform(df_axe2)\n",
        "\n",
        "        entropy_axe1 = calculate_entropy(X_scaled_axe1)\n",
        "        entropy_axe2 = calculate_entropy(X_scaled_axe2)\n",
        "\n",
        "        # Applico il TSNE con 1 componente (per ottenere un dataset unidimensionale).\n",
        "        # Gli iper-parametri (perplexity, learning_rate e n_iter) sono settati un po' ad\n",
        "        # occhio. Andrebbe fatta un po' di hyper-parameters optimization per vedere se è possibile\n",
        "        # ottenere valori ancora migliori di KL divergence (migliori = che danno un risultato di Percentage Entropy increase più basso).\n",
        "        tsne = TSNE(n_components=1, perplexity=30, learning_rate = 15.0, n_iter = 500)\n",
        "\n",
        "        X_tsne_axe1 = tsne.fit_transform(X_scaled_axe1)\n",
        "        kl_divergence_axe1 = tsne.kl_divergence_\n",
        "        perc_sum += (kl_divergence_axe1/entropy_axe1)*100\n",
        "\n",
        "        X_tsne_axe2 = tsne.fit_transform(X_scaled_axe2)\n",
        "        kl_divergence_axe2 = tsne.kl_divergence_\n",
        "        perc_sum += (kl_divergence_axe2/entropy_axe2)*100\n",
        "\n",
        "        df_schema = {\n",
        "            'Acquisition Number': np.full((1, X_tsne_axe1.size), acq_num, dtype=int)[0],\n",
        "            'time': np.arange(1, X_tsne_axe1.size + 1, 1, dtype=int),\n",
        "            'time_series_1': X_tsne_axe1.reshape(1,X_tsne_axe1.size)[0],\n",
        "            'time_series_2': X_tsne_axe2.reshape(1,X_tsne_axe2.size)[0]\n",
        "          }\n",
        "        df = pd.DataFrame(df_schema)\n",
        "        df = df.set_index(['Acquisition Number', 'time'])\n",
        "        print(\"-------------------------------------------------------------------------\")\n",
        "        print(df)\n",
        "\n",
        "        final_df = pd.concat([final_df, df], axis=0)\n",
        "\n",
        "    mean_perc_array[traino_number - 1] = perc_sum / (num_files * 2)\n",
        "\n",
        "    final_df.to_csv('data/tsne_data/TrainoCaso' + str(traino_number) + '/time_series_tsne.csv', sep=',', encoding='utf-8')\n",
        "\n",
        "mean_percs = {\n",
        "    'mean_perc_1': mean_perc_array[0],\n",
        "    'mean_perc_2': mean_perc_array[1]\n",
        "  }\n",
        "df = pd.DataFrame(mean_percs, index = [0])\n",
        "df.to_csv('data/tsne_data/entropy_perc.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "ej6JkZKwPG7F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}