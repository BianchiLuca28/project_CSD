{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meguazy/project_CSD/blob/main/notebook_exploration_cleaning/ingestion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "sDU0SpvewTeN",
        "outputId": "3725e953-2250-4037-a69c-e6bb9bf02889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'project_CSD'...\n",
            "remote: Enumerating objects: 541, done.\u001b[K\n",
            "remote: Counting objects: 100% (369/369), done.\u001b[K\n",
            "remote: Compressing objects: 100% (273/273), done.\u001b[K\n",
            "remote: Total 541 (delta 108), reused 334 (delta 83), pack-reused 172\u001b[K\n",
            "Receiving objects: 100% (541/541), 33.84 MiB | 25.16 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/Meguazy/project_CSD.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd project_CSD/"
      ],
      "metadata": {
        "id": "6mdYIoh0tI9k",
        "outputId": "e34e900c-6c24-41e1-bf6e-6b7631a345c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/project_CSD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Usare ogni volta che si inizia a lavorare per accertarsi che non ci siano\n",
        "#cambiamenti non sincronizzati\n",
        "\n",
        "!git pull"
      ],
      "metadata": {
        "id": "qRRxymcztAiQ",
        "outputId": "642a3ae3-84c6-4c42-d227-8467a48e071f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import requests\n",
        "gcloud_token = !gcloud auth print-access-token\n",
        "gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "EMAIL = str(gcloud_tokeninfo['email'])\n",
        "\n",
        "!echo $EMAIL\n",
        "\n",
        "#Usare per fare commit atomici e frequenti.\n",
        "#Ricordiamoci di usare mettere sempre dei messaggi di commit chiari in modo da\n",
        "#poter rollbackare o cherry-pickare in caso di bisogno.\n",
        "\n",
        "!git config --global user.email $EMAIL\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Added first pre-processing logic\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "rTwhriuWu7WZ",
        "outputId": "eff26ad1-c41b-480e-c111-1911f9f725a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "francy.finucci@gmail.com\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# FUNCTION FOR INGESTING THE ANAGRAFICA FILES\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "# folder path\n",
        "def ingest_anagrafica(dir_path):\n",
        "    final_anagrafica_df = pd.DataFrame()\n",
        "\n",
        "    for file_name in os.listdir(dir_path):\n",
        "\n",
        "        if os.path.isfile(os.path.join(dir_path, file_name)):\n",
        "\n",
        "            #Importing the raw dataframes\n",
        "            raw_dataframe = pd.read_excel(dir_path + \"/\" + file_name,nrows=6,header = None)\n",
        "\n",
        "            #Transposing the dataframe in order to have rows as columns.\n",
        "            #The reason behind that is that in the raw dataset the rows and columns are inverted.\n",
        "            T_table = raw_dataframe.T\n",
        "\n",
        "            new_header = T_table.iloc[0] #grab the first row for the header\n",
        "            T_table = T_table[1:] #take the data less the header row\n",
        "            T_table.columns = new_header #set the header row as the df header\n",
        "\n",
        "            anagrafica_df = T_table.set_index('Acquisition Number') #Set the new index\n",
        "\n",
        "            #Replacing the column \"Material\" that contains a string like \"d:n|r:m\", with n\n",
        "            #being the diameter of the iron bar and m being the type of iron (0 for nervato and 1 for non nervato).\n",
        "            #Here we put 'n' into the \"Material\" column and 'm' into the \"Diameter\" column.\n",
        "            diameter, material = str(anagrafica_df['Material'].values[0]).split('|')\n",
        "            anagrafica_df['Material_type'] = material.split(':')[1]\n",
        "            anagrafica_df['Diameter'] = diameter.split(':')[1]\n",
        "\n",
        "            #Dropping the original Material\n",
        "            anagrafica_df.drop([\"Material\"], axis=1, inplace=True)\n",
        "\n",
        "            final_anagrafica_df = pd.concat([final_anagrafica_df, anagrafica_df])\n",
        "\n",
        "            input(\"Inserisci per continuare: \")\n",
        "\n",
        "    return final_anagrafica_df"
      ],
      "metadata": {
        "id": "zWpIGq7stFof"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# FUNCTION FOR INGESTING THE TIME SERIES FILES\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def ingest_time_series(dir_path):\n",
        "    final_x = pd.DataFrame()\n",
        "    final_y = pd.DataFrame()\n",
        "    final_z = pd.DataFrame()\n",
        "\n",
        "    acq_list = []\n",
        "    count = 1\n",
        "    for file_name in os.listdir(dir_path):\n",
        "        #Extracting the acq_number\n",
        "        acq_number = str(re.findall(r'_\\d+', file_name)[0]).split(\"_\")[1]\n",
        "\n",
        "        if os.path.isfile(os.path.join(dir_path, file_name)):\n",
        "            #Ingesting the raw dataframe\n",
        "            raw_dataframe = pd.read_excel(dir_path + \"/\" + file_name,skiprows=6)\n",
        "\n",
        "            new_header = raw_dataframe.iloc[0] #grab the first row for the header\n",
        "            raw_dataframe = raw_dataframe[1:] #take the data less the header row\n",
        "            raw_dataframe.columns = new_header #set the header row as the df header\n",
        "\n",
        "            X = raw_dataframe[[\"Axe2X\"]]\n",
        "            Y = raw_dataframe[[\"Axe2Y\"]]\n",
        "            Z = raw_dataframe[[\"Axe2Z\"]]\n",
        "\n",
        "            sc = StandardScaler()\n",
        "            X = pd.DataFrame(sc.fit_transform(X)).T\n",
        "            Y = pd.DataFrame(sc.fit_transform(Y)).T\n",
        "            Z = pd.DataFrame(sc.fit_transform(Z)).T\n",
        "\n",
        "            acq_list.append(acq_number)\n",
        "\n",
        "            final_x = pd.concat([final_x, X], axis=0)\n",
        "            final_y = pd.concat([final_y, Y], axis=0)\n",
        "            final_z = pd.concat([final_z, Z], axis=0)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "    x = final_x.fillna(0)\n",
        "    y = final_y.fillna(0)\n",
        "    z = final_z.fillna(0)\n",
        "\n",
        "    x.insert(0, \"Acquisition Number\", acq_list)\n",
        "    y.insert(0, \"Acquisition Number\", acq_list)\n",
        "    z.insert(0, \"Acquisition Number\", acq_list)\n",
        "\n",
        "    x[\"Acquisition Number\"] = x[\"Acquisition Number\"].astype(int)\n",
        "    y[\"Acquisition Number\"] = y[\"Acquisition Number\"].astype(int)\n",
        "    z[\"Acquisition Number\"] = z[\"Acquisition Number\"].astype(int)\n",
        "\n",
        "    x.sort_values(by=\"Acquisition Number\", ascending = True, inplace = True)\n",
        "    y.sort_values(by=\"Acquisition Number\", ascending = True, inplace = True)\n",
        "    z.sort_values(by=\"Acquisition Number\", ascending = True, inplace = True)\n",
        "\n",
        "    x.set_index(\"Acquisition Number\", inplace = True)\n",
        "    y.set_index(\"Acquisition Number\", inplace = True)\n",
        "    z.set_index(\"Acquisition Number\", inplace = True)\n",
        "\n",
        "    return x,y,z"
      ],
      "metadata": {
        "id": "Nxc9ajLLUsDj"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING THE CSV FILES FOR THE CASE \"TrainoCaso1\"\n",
        "\n",
        "ingest_anagrafica(\"data/raw_data/DatasetRuoteCaso1\").to_csv('data/processed_data/TrainoCaso1/anagrafica.csv', sep=',', encoding='utf-8')\n",
        "ingest_time_series(\"data/raw_data/DatasetRuoteCaso1\").to_csv('data/processed_data/TrainoCaso1/time_series.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "0EJw9jqHXrRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y, z = ingest_time_series(\"data/raw_data/DatasetRuoteCaso1\")"
      ],
      "metadata": {
        "id": "n84navqJ0F58"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aa = np.sqrt((x*x)+(y*y)+(z*z))\n",
        "\n",
        "aa.to_csv('data/processed_data/Caso1SingleAxes/norm_TS2.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "1jxDkqF07GLS"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.to_csv('data/processed_data/Caso2SingleAxes/time_series_1X.csv', sep=',', encoding='utf-8')\n",
        "y.to_csv('data/processed_data/Caso2SingleAxes/time_series_1Y.csv', sep=',', encoding='utf-8')\n",
        "z.to_csv('data/processed_data/Caso2SingleAxes/time_series_1Z.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "BO9D7wjJ427-"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.to_csv('data/processed_data/Caso2SingleAxes/time_series_2X.csv', sep=',', encoding='utf-8')\n",
        "y.to_csv('data/processed_data/Caso2SingleAxes/time_series_2Y.csv', sep=',', encoding='utf-8')\n",
        "z.to_csv('data/processed_data/Caso2SingleAxes/time_series_2Z.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "XO5IksB0_r8a"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING THE CSV FILES FOR THE CASE \"TrainoCaso2\"\n",
        "\n",
        "ingest_anagrafica(\"data/raw_data/DatasetRuoteCaso2\").to_csv('data/processed_data/TrainoCaso2/anagrafica.csv', sep=',', encoding='utf-8')\n",
        "ingest_time_series(\"data/raw_data/DatasetRuoteCaso2\").to_csv('data/processed_data/TrainoCaso2/time_series.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "0l_mOtLRC2rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "80vVpxjLECOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = pd.read_csv(\"data/processed_data/TrainoCaso1/time_series.csv\", sep=',', encoding='utf-8')\n",
        "\n",
        "sc = StandardScaler()\n",
        "a = pd.DataFrame(sc.fit_transform(a[[\"Axe1X\", \"Axe1Y\", \"Axe1Z\", \"Axe2X\", \"Axe2Y\", \"Axe2Z\"]]), columns=[\"Axe1X\", \"Axe1Y\", \"Axe1Z\", \"Axe2X\", \"Axe2Y\", \"Axe2Z\"])\n",
        "\n",
        "print(a)\n",
        "\n",
        "a[\"Axe1X\"] = pd.to_numeric(a[\"Axe1X\"])\n",
        "a[\"Axe1Y\"] = pd.to_numeric(a[\"Axe1Y\"])\n",
        "a[\"Axe1Z\"] = pd.to_numeric(a[\"Axe1Z\"])\n",
        "a[\"Axe2X\"] = pd.to_numeric(a[\"Axe2X\"])\n",
        "a[\"Axe2Y\"] = pd.to_numeric(a[\"Axe2Y\"])\n",
        "a[\"Axe2Z\"] = pd.to_numeric(a[\"Axe2Z\"])\n",
        "\n",
        "a.abs().mean(axis=0)"
      ],
      "metadata": {
        "id": "tqC0gfMs3uzE",
        "outputId": "562fd589-d80a-449c-aeb3-be956a34ba70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Axe1X     Axe1Y     Axe1Z     Axe2X     Axe2Y     Axe2Z\n",
            "0      -1.152438  0.982009  2.154811 -0.506651 -1.257924 -1.350312\n",
            "1       2.018981 -0.934781  1.926032  0.061700 -0.667042  4.419507\n",
            "2       1.389886 -0.775463  0.653449 -2.417486  1.325931  0.531973\n",
            "3       0.608381  5.353286 -4.353134 -3.710975 -0.697087 -2.381312\n",
            "4       0.096025 -2.139620  0.289854  3.275824 -0.226385 -2.797496\n",
            "...          ...       ...       ...       ...       ...       ...\n",
            "114769  0.618109 -0.237766 -0.659987  0.306679  0.023989 -0.167972\n",
            "114770  0.575953 -0.382148  0.383817 -0.036291  0.364497  0.560349\n",
            "114771 -0.779520  0.080869  0.490036 -0.242073 -0.526833  0.248212\n",
            "114772 -0.267164 -0.202915 -0.036973  0.042102  0.254332  0.059037\n",
            "114773  0.420301  0.160528 -0.329075 -0.144082  0.174213  0.191459\n",
            "\n",
            "[114774 rows x 6 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Axe1X    0.694473\n",
              "Axe1Y    0.684165\n",
              "Axe1Z    0.692237\n",
              "Axe2X    0.736536\n",
              "Axe2Y    0.729210\n",
              "Axe2Z    0.734363\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    }
  ]
}
