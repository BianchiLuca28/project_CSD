{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meguazy/project_CSD/blob/main/notebook_exploration_cleaning/ingestion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDU0SpvewTeN"
      },
      "outputs": [],
      "source": [
        "!git clone https://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@github.com/Meguazy/project_CSD.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd project_CSD/"
      ],
      "metadata": {
        "id": "6mdYIoh0tI9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Usare ogni volta che si inizia a lavorare per accertarsi che non ci siano\n",
        "#cambiamenti non sincronizzati\n",
        "\n",
        "!git pull"
      ],
      "metadata": {
        "id": "qRRxymcztAiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import requests\n",
        "gcloud_token = !gcloud auth print-access-token\n",
        "gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "EMAIL = str(gcloud_tokeninfo['email'])\n",
        "\n",
        "!echo $EMAIL\n",
        "\n",
        "#Usare per fare commit atomici e frequenti.\n",
        "#Ricordiamoci di usare mettere sempre dei messaggi di commit chiari in modo da\n",
        "#poter rollbackare o cherry-pickare in caso di bisogno.\n",
        "\n",
        "!git config --global user.email $EMAIL\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Added first pre-processing logic\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "rTwhriuWu7WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# FUNCTION FOR INGESTING THE ANAGRAFICA FILES\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "# folder path\n",
        "def ingest_anagrafica(dir_path):\n",
        "    final_anagrafica_df = pd.DataFrame()\n",
        "\n",
        "    for file_name in os.listdir(dir_path):\n",
        "\n",
        "        if os.path.isfile(os.path.join(dir_path, file_name)):\n",
        "\n",
        "            #Importing the raw dataframes\n",
        "            raw_dataframe = pd.read_excel(dir_path + \"/\" + file_name,nrows=6,header = None)\n",
        "\n",
        "            #Transposing the dataframe in order to have rows as columns.\n",
        "            #The reason behind that is that in the raw dataset the rows and columns are inverted.\n",
        "            T_table = raw_dataframe.T\n",
        "\n",
        "            new_header = T_table.iloc[0] #grab the first row for the header\n",
        "            T_table = T_table[1:] #take the data less the header row\n",
        "            T_table.columns = new_header #set the header row as the df header\n",
        "\n",
        "            anagrafica_df = T_table.set_index('Acquisition Number') #Set the new index\n",
        "\n",
        "            #Replacing the column \"Material\" that contains a string like \"d:n|r:m\", with n\n",
        "            #being the diameter of the iron bar and m being the type of iron (0 for nervato and 1 for non nervato).\n",
        "            #Here we put 'n' into the \"Material\" column and 'm' into the \"Diameter\" column.\n",
        "            diameter, material = str(anagrafica_df['Material'].values[0]).split('|')\n",
        "            anagrafica_df['Material_type'] = material.split(':')[1]\n",
        "            anagrafica_df['Diameter'] = diameter.split(':')[1]\n",
        "\n",
        "            #Dropping the original Material\n",
        "            anagrafica_df.drop([\"Material\"], axis=1, inplace=True)\n",
        "\n",
        "            final_anagrafica_df = pd.concat([final_anagrafica_df, anagrafica_df])\n",
        "\n",
        "    return final_anagrafica_df"
      ],
      "metadata": {
        "id": "zWpIGq7stFof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# FUNCTION FOR INGESTING THE TIME SERIES FILES\n",
        "\n",
        "def ingest_time_series(dir_path):\n",
        "    final_time_series_df = pd.DataFrame()\n",
        "\n",
        "    for file_name in os.listdir(dir_path):\n",
        "        #Extracting the acq_number\n",
        "        acq_number = str(re.findall(r'_\\d+', file_name)[0]).split(\"_\")[1]\n",
        "        print(acq_number)\n",
        "\n",
        "        if os.path.isfile(os.path.join(dir_path, file_name)):\n",
        "            #Ingesting the raw dataframe\n",
        "            raw_dataframe = pd.read_excel(dir_path + \"/\" + file_name,skiprows=6)\n",
        "\n",
        "            new_header = raw_dataframe.iloc[0] #grab the first row for the header\n",
        "            raw_dataframe = raw_dataframe[1:] #take the data less the header row\n",
        "            raw_dataframe.columns = new_header #set the header row as the df header\n",
        "\n",
        "            raw_dataframe['Acquisition Number'] = acq_number\n",
        "            time_series_df = raw_dataframe.set_index(['Acquisition Number','Time'])\n",
        "\n",
        "            final_time_series_df = pd.concat([final_time_series_df, time_series_df], axis=0)\n",
        "\n",
        "    #new_df.loc[new_df['Acquisition Number'] == 1]\n",
        "\n",
        "    return final_time_series_df"
      ],
      "metadata": {
        "id": "Nxc9ajLLUsDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING THE CSV FILES FOR THE CASE \"TrainoCaso1\"\n",
        "\n",
        "ingest_anagrafica(\"data/raw_data/DatasetRuoteCaso1\").to_csv('data/processed_data/TrainoCaso1/anagrafica.csv', sep=',', encoding='utf-8')\n",
        "ingest_time_series(\"data/raw_data/DatasetRuoteCaso1\").to_csv('data/processed_data/TrainoCaso1/time_series.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "0EJw9jqHXrRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING THE CSV FILES FOR THE CASE \"TrainoCaso2\"\n",
        "\n",
        "ingest_anagrafica(\"data/raw_data/DatasetRuoteCaso2\").to_csv('data/processed_data/TrainoCaso2/anagrafica.csv', sep=',', encoding='utf-8')\n",
        "ingest_time_series(\"data/raw_data/DatasetRuoteCaso2\").to_csv('data/processed_data/TrainoCaso2/time_series.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "0l_mOtLRC2rs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}