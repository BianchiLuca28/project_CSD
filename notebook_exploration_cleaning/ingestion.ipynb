{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meguazy/project_CSD/blob/main/notebook_exploration_cleaning/ingestion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDU0SpvewTeN"
      },
      "outputs": [],
      "source": [
        "!git clone https://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@github.com/Meguazy/project_CSD.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd project_CSD/"
      ],
      "metadata": {
        "id": "6mdYIoh0tI9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Usare ogni volta che si inizia a lavorare per accertarsi che non ci siano\n",
        "#cambiamenti non sincronizzati\n",
        "\n",
        "!git pull"
      ],
      "metadata": {
        "id": "qRRxymcztAiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import requests\n",
        "gcloud_token = !gcloud auth print-access-token\n",
        "gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "EMAIL = str(gcloud_tokeninfo['email'])\n",
        "\n",
        "!echo $EMAIL\n",
        "\n",
        "#Usare per fare commit atomici e frequenti.\n",
        "#Ricordiamoci di usare mettere sempre dei messaggi di commit chiari in modo da\n",
        "#poter rollbackare o cherry-pickare in caso di bisogno.\n",
        "\n",
        "!git config --global user.email $EMAIL\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Added first pre-processing logic\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "rTwhriuWu7WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# FUNCTION FOR INGESTING THE ANAGRAFICA FILES\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "# folder path\n",
        "def ingest_anagrafica(dir_path):\n",
        "    final_anagrafica_df = pd.DataFrame()\n",
        "\n",
        "    for file_name in os.listdir(dir_path):\n",
        "\n",
        "        if os.path.isfile(os.path.join(dir_path, file_name)):\n",
        "\n",
        "            #Importing the raw dataframes\n",
        "            raw_dataframe = pd.read_excel(dir_path + \"/\" + file_name,nrows=6,header = None)\n",
        "\n",
        "            #Transposing the dataframe in order to have rows as columns.\n",
        "            #The reason behind that is that in the raw dataset the rows and columns are inverted.\n",
        "            T_table = raw_dataframe.T\n",
        "\n",
        "            new_header = T_table.iloc[0] #grab the first row for the header\n",
        "            T_table = T_table[1:] #take the data less the header row\n",
        "            T_table.columns = new_header #set the header row as the df header\n",
        "\n",
        "            anagrafica_df = T_table.set_index('Acquisition Number') #Set the new index\n",
        "\n",
        "            #Replacing the column \"Material\" that contains a string like \"d:n|r:m\", with n\n",
        "            #being the diameter of the iron bar and m being the type of iron (0 for nervato and 1 for non nervato).\n",
        "            #Here we put 'n' into the \"Material\" column and 'm' into the \"Diameter\" column.\n",
        "            diameter, material = str(anagrafica_df['Material'].values[0]).split('|')\n",
        "            anagrafica_df['Material_type'] = material.split(':')[1]\n",
        "            anagrafica_df['Diameter'] = diameter.split(':')[1]\n",
        "\n",
        "            #Dropping the original Material\n",
        "            anagrafica_df.drop([\"Material\"], axis=1, inplace=True)\n",
        "\n",
        "            final_anagrafica_df = pd.concat([final_anagrafica_df, anagrafica_df])\n",
        "\n",
        "    return final_anagrafica_df"
      ],
      "metadata": {
        "id": "zWpIGq7stFof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# FUNCTION FOR INGESTING THE TIME SERIES FILES\n",
        "\n",
        "def ingest_time_series(dir_path):\n",
        "    final_time_series_df = pd.DataFrame()\n",
        "\n",
        "    for file_name in os.listdir(dir_path):\n",
        "        #Extracting the acq_number\n",
        "        acq_number = str(re.findall(r'_\\d+', file_name)[0]).split(\"_\")[1]\n",
        "        print(acq_number)\n",
        "\n",
        "        if os.path.isfile(os.path.join(dir_path, file_name)):\n",
        "            #Ingesting the raw dataframe\n",
        "            raw_dataframe = pd.read_excel(dir_path + \"/\" + file_name,skiprows=6)\n",
        "\n",
        "            new_header = raw_dataframe.iloc[0] #grab the first row for the header\n",
        "            raw_dataframe = raw_dataframe[1:] #take the data less the header row\n",
        "            raw_dataframe.columns = new_header #set the header row as the df header\n",
        "\n",
        "            raw_dataframe['Acquisition Number'] = acq_number\n",
        "            time_series_df = raw_dataframe.set_index(['Acquisition Number','Time'])\n",
        "\n",
        "            final_time_series_df = pd.concat([final_time_series_df, time_series_df], axis=0)\n",
        "\n",
        "    #new_df.loc[new_df['Acquisition Number'] == 1]\n",
        "\n",
        "    return final_time_series_df"
      ],
      "metadata": {
        "id": "Nxc9ajLLUsDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING THE CSV FILES FOR THE CASE \"TrainoCaso1\"\n",
        "\n",
        "ingest_anagrafica(\"data/raw_data/DatasetRuoteCaso1\").to_csv('data/processed_data/TrainoCaso1/anagrafica.csv', sep=',', encoding='utf-8')\n",
        "ingest_time_series(\"data/raw_data/DatasetRuoteCaso1\").to_csv('data/processed_data/TrainoCaso1/time_series.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "0EJw9jqHXrRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVING THE CSV FILES FOR THE CASE \"TrainoCaso2\"\n",
        "\n",
        "ingest_anagrafica(\"data/raw_data/DatasetRuoteCaso2\").to_csv('data/processed_data/TrainoCaso2/anagrafica.csv', sep=',', encoding='utf-8')\n",
        "ingest_time_series(\"data/raw_data/DatasetRuoteCaso2\").to_csv('data/processed_data/TrainoCaso2/time_series.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "0l_mOtLRC2rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# APPLICO IL T-SNE E CALCOLO LA KL-DIVERGENCE\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "time_series_df = pd.read_csv(\"data/processed_data/TrainoCaso1/time_series.csv\")\n",
        "\n",
        "# Seleziono, in questo caso, l'acquisizione numero 60\n",
        "time_series_df = time_series_df.loc[time_series_df['Acquisition Number'] == 60]\n",
        "# Facendo così prendo solo la prima time series dell'acquisizione.\n",
        "selected_df = time_series_df[['Axe1X', 'Axe1Y', 'Axe1Z']]\n",
        "# Se volessi prendere la seconda (perché ricordiamo che abbiamo 2 time series ad acquisizione)\n",
        "# avrei dovuto scrivere la seguente linea di codice:\n",
        "# selected_df = time_series_df[['Axe2X', 'Axe2Y', 'Axe2Z']]\n",
        "# Da tenere a mente che il dataset finale dovrà contenere ENTRAMBE le time series sullo stesso record.\n",
        "# Quindi, la struttura finale della tabella dovrà essere:\n",
        "# [time, time_series_1, time_series_2]\n",
        "# Mentre quella attuale è\n",
        "# [time, 'Axe1X', 'Axe1Y', 'Axe1Z', 'Axe2X', 'Axe2Y', 'Axe2Z']\n",
        "# Per schematizzare ancora meglio:\n",
        "# 1) 'Axe1X', 'Axe1Y', 'Axe1Z' ---> (tramite il t-sne) time_series_1\n",
        "# 2) 'Axe2X', 'Axe2Y', 'Axe2Z' ---> (tramite il t-sne) time_series_2\n",
        "\n",
        "# Normalizzo i dati\n",
        "sc = StandardScaler()\n",
        "X_scaled = sc.fit_transform(selected_df)\n",
        "\n",
        "# Applico il TSNE con 1 componente (per ottenere un dataset unidimensionale).\n",
        "# Gli iper-parametri (perplexity, learning_rate e n_iter) sono settati un po' ad\n",
        "# occhio. Andrebbe fatta un po' di hyper-parameters optimization per vedere se è possibile\n",
        "# ottenere valori ancora migliori di KL divergence (migliori = che danno un risultato di Percentage Entropy increase più basso).\n",
        "tsne = TSNE(n_components=1, perplexity=30, learning_rate = 15.0, n_iter = 2000)\n",
        "X_tsne = tsne.fit_transform(X_scaled)\n",
        "\n",
        "kl_divergence = tsne.kl_divergence_"
      ],
      "metadata": {
        "id": "ej6JkZKwPG7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CALCOLO L'ENTROPIA TOTALE DEL DATASET DI PARTENZA\n",
        "\n",
        "from scipy.stats import entropy\n",
        "\n",
        "print(X_scaled)\n",
        "\n",
        "# Setto una costante in modo da avere il meno possibile probabilità uguali a 0.\n",
        "# Questo perché scipy.stats.entropy assume che il vettore passato non abbia\n",
        "# probabilità uguali a 0.\n",
        "epsilon = 0.1\n",
        "\n",
        "# Modifico il dataset \"X_scaled\" (quello iniziale) aggiungendo epsilon\n",
        "joint_probs = X_scaled.flatten() + epsilon\n",
        "print(joint_probs)\n",
        "print(f\"size joint_probs {joint_probs.size}\")\n",
        "\n",
        "# In alcuni casi, per valori troppo vicini allo zero, python setta\n",
        "# automaticamente a 0 la variabile anche dopo aver aggiunto epsilon.\n",
        "# Dato che le probabilità a zero non hanno alcun peso sull'entropia di un\n",
        "# oggetto, semplicemente le filtro.\n",
        "nonzero_probs = joint_probs[joint_probs > 0]\n",
        "print(f\"size nonzero_probs {nonzero_probs.size}\")\n",
        "\n",
        "if len(nonzero_probs) == 0:\n",
        "    print(\"Tutte le probabilità sono zero. Impossibile calcolare l'entropia\")\n",
        "else:\n",
        "    # base=2 significa che sto usando logaritmi in base 2 per calcolare l'entropia\n",
        "    # Possono essere usati anche altri logaritmi, ma la base 2 è quella usata dal\n",
        "    # t-sne. Ergo, dobbiamo restare sulla stessa base di logaritmo anche per il totale.\n",
        "    joint_entropy = entropy(nonzero_probs, base=2)\n",
        "    print(f\"Joint Entropy: {joint_entropy}\")"
      ],
      "metadata": {
        "id": "GRNBKDGuiVVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CALCOLO il Percentage Entropy increase\n",
        "\n",
        "print(f\"KL divergence: {kl_divergence}\")\n",
        "print(f\"Initial dataset entropy: {joint_entropy}\")\n",
        "\n",
        "# Qui calcolo l'aumento percentuale di entropia come divisione di KL per entropia totale * 100\n",
        "print(f\"Percentage Entropy increase: {(kl_divergence/joint_entropy)*100}%\")"
      ],
      "metadata": {
        "id": "C2uPDVYOj5M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,6))\n",
        "plt.plot(np.arange(0, X_tsne.size, 1), X_tsne)"
      ],
      "metadata": {
        "id": "GGb8PQ6EdfLl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}