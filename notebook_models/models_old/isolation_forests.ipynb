{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkPJzpiL-wwI",
        "outputId": "d98664bd-8ac1-4925-9033-cb3cc313f2b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'project_CSD'...\n",
            "remote: Enumerating objects: 583, done.\u001b[K\n",
            "remote: Counting objects: 100% (411/411), done.\u001b[K\n",
            "remote: Compressing objects: 100% (314/314), done.\u001b[K\n",
            "remote: Total 583 (delta 121), reused 356 (delta 84), pack-reused 172\u001b[K\n",
            "Receiving objects: 100% (583/583), 45.48 MiB | 22.91 MiB/s, done.\n",
            "Resolving deltas: 100% (143/143), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/Meguazy/project_CSD.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd project_CSD/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD4RvSXU--9P",
        "outputId": "003aefc1-165b-47db-edf0-2d93acecf4fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/project_CSD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import requests\n",
        "gcloud_token = !gcloud auth print-access-token\n",
        "gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "EMAIL = str(gcloud_tokeninfo['email'])\n",
        "\n",
        "!echo $EMAIL\n",
        "\n",
        "#Usare per fare commit atomici e frequenti.\n",
        "#Ricordiamoci di usare mettere sempre dei messaggi di commit chiari in modo da\n",
        "#poter rollbackare o cherry-pickare in caso di bisogno.\n",
        "\n",
        "!git config --global user.email $EMAIL\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Added sparse autoencored algorithm\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "cGjyP-O1-_MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "0i635hx5-_Q2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load normal and anomalous datasets\n",
        "normal_data_path = 'data/processed_data/TrainoCaso1/time_series.csv'\n",
        "anomalous_data_path = 'data/processed_data/TrainoCaso2/time_series.csv'"
      ],
      "metadata": {
        "id": "k_YWOqym-_Tv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "normal_data = pd.read_csv(normal_data_path)\n",
        "anomalous_data = pd.read_csv(anomalous_data_path)"
      ],
      "metadata": {
        "id": "HEVhaeGm-_WQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine normal and anomalous data into one dataset\n",
        "all_data = pd.concat([normal_data, anomalous_data], axis=0)"
      ],
      "metadata": {
        "id": "NXQF3S5J-_YU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the columns representing time series data\n",
        "time_series_columns = ['Axe1X', 'Axe1Y', 'Axe1Z', 'Axe2X', 'Axe2Y', 'Axe2Z']"
      ],
      "metadata": {
        "id": "rgyubu6--_c3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the time series data within each acquisition number group\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "y5FjKzow-_fV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the transformation to each group separately\n",
        "transformed_data_list = []\n",
        "for _, group in all_data.groupby('Acquisition Number'):\n",
        "    transformed_data = scaler.fit_transform(group[time_series_columns])\n",
        "    transformed_data_list.append(pd.DataFrame(transformed_data, columns=time_series_columns, index=group.index))"
      ],
      "metadata": {
        "id": "pAryABVW-_h6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the transformed data with 'Acquisition Number' and 'time'\n",
        "all_data_scaled = pd.concat([all_data[['Acquisition Number', 'Time']].reset_index(drop=True), pd.concat(transformed_data_list).reset_index(drop=True)], axis=1)"
      ],
      "metadata": {
        "id": "2COdxymW-_kO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(all_data_scaled, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "i_s1LI1G-_mx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Isolation Forest model on the normal data\n",
        "model = IsolationForest(contamination=0.05, random_state=42)\n",
        "model.fit(train_data[time_series_columns])"
      ],
      "metadata": {
        "id": "xmvQ7Mgo-_pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict outliers in both normal and anomalous test data\n",
        "predicted_normal = model.predict(test_data[time_series_columns])\n",
        "predicted_anomalous = model.predict(anomalous_data[time_series_columns])"
      ],
      "metadata": {
        "id": "DOELF2pZ-_rc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to binary (1 for inliers, -1 for outliers)\n",
        "predicted_normal_binary = np.where(predicted_normal == 1, 0, 1)\n",
        "predicted_anomalous_binary = np.where(predicted_anomalous == 1, 0, 1)"
      ],
      "metadata": {
        "id": "4Sr4lN-R_oZF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(np.concatenate([np.zeros(len(predicted_normal_binary)), np.ones(len(predicted_anomalous_binary))]),\n",
        "                            np.concatenate([predicted_normal_binary, predicted_anomalous_binary]))\n",
        "recall = recall_score(np.concatenate([np.zeros(len(predicted_normal_binary)), np.ones(len(predicted_anomalous_binary))]),\n",
        "                      np.concatenate([predicted_normal_binary, predicted_anomalous_binary]))\n",
        "f1 = f1_score(np.concatenate([np.zeros(len(predicted_normal_binary)), np.ones(len(predicted_anomalous_binary))]),\n",
        "               np.concatenate([predicted_normal_binary, predicted_anomalous_binary]))\n"
      ],
      "metadata": {
        "id": "7jbSsGpw_oga"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print or visualize the results\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "f79urhw0_olQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
