{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMEBT4MFSgac"
      },
      "outputs": [],
      "source": [
        "!git clone https://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@github.com/Meguazy/project_CSD.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd project_CSD/"
      ],
      "metadata": {
        "id": "F5VmOw9sSpai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Usare ogni volta che si inizia a lavorare per accertarsi che non ci siano\n",
        "#cambiamenti non sincronizzati\n",
        "\n",
        "!git pull"
      ],
      "metadata": {
        "id": "dDGvKZZISu3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import requests\n",
        "gcloud_token = !gcloud auth print-access-token\n",
        "gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "EMAIL = str(gcloud_tokeninfo['email'])\n",
        "\n",
        "!echo $EMAIL\n",
        "\n",
        "#Usare per fare commit atomici e frequenti.\n",
        "#Ricordiamoci di usare mettere sempre dei messaggi di commit chiari in modo da\n",
        "#poter rollbackare o cherry-pickare in caso di bisogno.\n",
        "\n",
        "!git config --global user.email $EMAIL\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Added sparse autoencored algorithm\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "Oh1MFDbOTFjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "-hMcmlGYlm64"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load normal and anomalous datasets\n",
        "normal_data_path = 'data/processed_data/TrainoCaso1/time_series.csv'\n",
        "anomalous_data_path = 'data/processed_data/TrainoCaso2/time_series.csv'"
      ],
      "metadata": {
        "id": "rDO_eWuPqagl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "normal_data = pd.read_csv(normal_data_path)\n",
        "anomalous_data = pd.read_csv(anomalous_data_path)"
      ],
      "metadata": {
        "id": "YZxREh08r2d9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine normal and anomalous data into one dataset\n",
        "all_data = pd.concat([normal_data, anomalous_data], axis=0)"
      ],
      "metadata": {
        "id": "bpkIEKNFsDVG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the columns representing time series data\n",
        "time_series_columns = ['Axe1X', 'Axe1Y', 'Axe1Z', 'Axe2X', 'Axe2Y', 'Axe2Z']"
      ],
      "metadata": {
        "id": "gVNlGm9OsKGV"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the time series data within each acquisition number group\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "BKqM1cGTzHMB"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the transformation to each group separately\n",
        "transformed_data_list = []\n",
        "for _, group in all_data.groupby('Acquisition Number'):\n",
        "    transformed_data = scaler.fit_transform(group[time_series_columns])\n",
        "    transformed_data_list.append(pd.DataFrame(transformed_data, columns=time_series_columns, index=group.index))"
      ],
      "metadata": {
        "id": "WxdcG6LW5CGu"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the transformed data with 'Acquisition Number' and 'time'\n",
        "all_data_scaled = pd.concat([all_data[['Acquisition Number', 'Time']].reset_index(drop=True), pd.concat(transformed_data_list).reset_index(drop=True)], axis=1)"
      ],
      "metadata": {
        "id": "1OQVcfEAtjar"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(all_data_scaled, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "49Ll925_vWn8"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sparse autoencoder model\n",
        "input_dim = len(time_series_columns)\n",
        "hidden_units = 3  # Adjust the number of neurons in the hidden layer as needed\n",
        "sparsity_penalty = 0.01  # Adjust as needed\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(units=hidden_units, activation='sigmoid', input_dim=input_dim,\n",
        "          activity_regularizer=regularizers.l1(sparsity_penalty)),\n",
        "    Dense(units=input_dim, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "KK8sl0Q2vZ4Z"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "ts_zj_Ifvdj0"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on normal data\n",
        "model.fit(train_data[time_series_columns], train_data[time_series_columns],\n",
        "          epochs=50, batch_size=32, shuffle=True, validation_split=0.1)"
      ],
      "metadata": {
        "id": "x6cVla4dvf-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstruct both normal and anomalous test data\n",
        "reconstructed_normal_data = model.predict(normal_data[time_series_columns])\n",
        "reconstructed_anomalous_data = model.predict(anomalous_data[time_series_columns])"
      ],
      "metadata": {
        "id": "vJMjCCdS13aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate reconstruction error for both sets\n",
        "mse_normal = np.mean(np.square(normal_data[time_series_columns] - reconstructed_normal_data), axis=1)\n",
        "mse_anomalous = np.mean(np.square(anomalous_data[time_series_columns] - reconstructed_anomalous_data), axis=1)"
      ],
      "metadata": {
        "id": "N3lerMqAyChM"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine a threshold for anomaly detection (e.g., based on percentile of normal data)\n",
        "threshold = np.percentile(mse_normal, 95)"
      ],
      "metadata": {
        "id": "VhtwKrX1yO0t"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify anomalies in both normal and anomalous test data\n",
        "predicted_normal_anomalies = normal_data[mse_normal > threshold]\n",
        "predicted_anomalous_anomalies = anomalous_data[mse_anomalous > threshold]"
      ],
      "metadata": {
        "id": "lFBnb2C7yRWy"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print or visualize the detected anomalies\n",
        "print(\"Predicted Anomalies in Normal Data:\")\n",
        "print(predicted_normal_anomalies)\n",
        "\n",
        "print(\"\\nPredicted Anomalies in Anomalous Data:\")\n",
        "print(predicted_anomalous_anomalies)"
      ],
      "metadata": {
        "id": "-164ZT_SySWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate True Positives and False Positives\n",
        "true_positives = np.sum(mse_anomalous > threshold)\n",
        "false_positives = np.sum(mse_normal > threshold)"
      ],
      "metadata": {
        "id": "vqHCBovIz_V2"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Precision\n",
        "precision = true_positives / (true_positives + false_positives)"
      ],
      "metadata": {
        "id": "7iPTwpXq0DJg"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Precision\n",
        "print(f'Precision: {precision:.2f}')"
      ],
      "metadata": {
        "id": "yiQveWRx0Fe_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
