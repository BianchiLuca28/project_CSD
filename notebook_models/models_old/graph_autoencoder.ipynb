{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meguazy/project_CSD/blob/main/notebook_models/graph_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmQc5taV2xlq",
        "outputId": "5114612d-a255-4ce1-a728-8e153db9ca7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'project_CSD'...\n",
            "remote: Enumerating objects: 641, done.\u001b[K\n",
            "remote: Counting objects: 100% (469/469), done.\u001b[K\n",
            "remote: Compressing objects: 100% (365/365), done.\u001b[K\n",
            "remote: Total 641 (delta 155), reused 366 (delta 91), pack-reused 172\u001b[K\n",
            "Receiving objects: 100% (641/641), 48.34 MiB | 23.24 MiB/s, done.\n",
            "Resolving deltas: 100% (178/178), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://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@github.com/Meguazy/project_CSD.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd project_CSD/"
      ],
      "metadata": {
        "id": "QM_sb6Ah20hq",
        "outputId": "14dd1a35-8d7b-4bc4-9b0a-48acbbc3d703",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/project_CSD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Usare ogni volta che si inizia a lavorare per accertarsi che non ci siano\n",
        "#cambiamenti non sincronizzati\n",
        "\n",
        "!git pull"
      ],
      "metadata": {
        "id": "LNG_lS_t22hx",
        "outputId": "3d531d90-f1d0-48d8-e970-e4a488c326c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import requests\n",
        "gcloud_token = !gcloud auth print-access-token\n",
        "gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
        "EMAIL = str(gcloud_tokeninfo['email'])\n",
        "\n",
        "!echo $EMAIL\n",
        "\n",
        "#Usare per fare commit atomici e frequenti.\n",
        "#Ricordiamoci di usare mettere sempre dei messaggi di commit chiari in modo da\n",
        "#poter rollbackare o cherry-pickare in caso di bisogno.\n",
        "\n",
        "!git config --global user.email $EMAIL\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "rOXTaZTS23ED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph autoencoder using spektral, which is the tensorflow library for graph neural networks\n",
        "Sample"
      ],
      "metadata": {
        "id": "xYjQ0BzG1V-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "wLp7jKv20KYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a sample graph with anomalies\n",
        "def generate_sample_graph(num_nodes=10, anomalous_node=None):\n",
        "    adjacency_matrix = np.random.randint(2, size=(num_nodes, num_nodes))\n",
        "    adjacency_matrix = np.triu(adjacency_matrix, k=1) + np.triu(adjacency_matrix, k=1).T\n",
        "    np.fill_diagonal(adjacency_matrix, 0)\n",
        "\n",
        "    # Introduce anomaly by changing connections of a specific node\n",
        "    if anomalous_node is not None:\n",
        "        adjacency_matrix[anomalous_node] = np.random.randint(2, size=num_nodes)\n",
        "\n",
        "    return adjacency_matrix.astype(np.float32)"
      ],
      "metadata": {
        "id": "C-L_DvSB0Zeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate a dataset with normal and anomalous graphs\n",
        "def generate_dataset(num_samples, num_nodes=10, anomalous_node=None):\n",
        "    X = [generate_sample_graph(num_nodes, anomalous_node) for _ in range(num_samples)]\n",
        "    return np.array(X)"
      ],
      "metadata": {
        "id": "Bll_Kh1u1rOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph Autoencoder Model with TensorFlow's Dense layers\n",
        "def create_graph_autoencoder(input_dim):\n",
        "    encoder_inputs = tf.keras.Input(shape=(input_dim, input_dim))\n",
        "    x = layers.Flatten()(encoder_inputs)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    x = layers.Dense(16, activation='relu')(x)\n",
        "    encoder_outputs = layers.Dense(8, activation='relu')(x)\n",
        "\n",
        "    decoder_inputs = tf.keras.Input(shape=(8,))\n",
        "    x = layers.Dense(16, activation='relu')(decoder_inputs)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    decoder_outputs = layers.Reshape((input_dim, input_dim))(layers.Dense(input_dim * input_dim, activation='sigmoid')(x))\n",
        "\n",
        "    encoder = tf.keras.Model(encoder_inputs, encoder_outputs, name='encoder')\n",
        "    decoder = tf.keras.Model(decoder_inputs, decoder_outputs, name='decoder')\n",
        "\n",
        "    autoencoder_inputs = tf.keras.Input(shape=(input_dim, input_dim))\n",
        "    autoencoder_outputs = decoder(encoder(autoencoder_inputs))\n",
        "\n",
        "    autoencoder = tf.keras.Model(autoencoder_inputs, autoencoder_outputs, name='autoencoder')\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    return autoencoder"
      ],
      "metadata": {
        "id": "e5Fj3-eS1tup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "num_nodes = 10\n",
        "num_samples = 1000\n",
        "anomalous_node = 5  # Introduce anomaly in node 5"
      ],
      "metadata": {
        "id": "0j4ex63i1yF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dataset\n",
        "X = generate_dataset(num_samples, num_nodes, anomalous_node)"
      ],
      "metadata": {
        "id": "GomvktTZ115t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create graph autoencoder\n",
        "autoencoder = create_graph_autoencoder(num_nodes)"
      ],
      "metadata": {
        "id": "Y2TojNHh179v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Training Loop\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    np.random.shuffle(X)\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        batch = X[i:i+batch_size]\n",
        "        autoencoder.train_on_batch(batch, batch)"
      ],
      "metadata": {
        "id": "x4BsvwiW2ECD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on normal and anomalous samples\n",
        "normal_sample = generate_sample_graph(num_nodes)\n",
        "anomalous_sample = generate_sample_graph(num_nodes, anomalous_node)"
      ],
      "metadata": {
        "id": "7nydnDtY2fvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_sample = np.expand_dims(normal_sample, axis=0)\n",
        "anomalous_sample = np.expand_dims(anomalous_sample, axis=0)"
      ],
      "metadata": {
        "id": "H2Dfe5C62hb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on normal sample\n",
        "reconstructed_normal = autoencoder.predict(normal_sample)\n",
        "\n",
        "# Predict on anomalous sample\n",
        "reconstructed_anomalous = autoencoder.predict(anomalous_sample)\n",
        "\n",
        "# Calculate reconstruction errors\n",
        "error_normal = np.mean(np.abs(normal_sample - reconstructed_normal))\n",
        "error_anomalous = np.mean(np.abs(anomalous_sample - reconstructed_anomalous))\n",
        "\n",
        "print(\"Reconstruction error on normal sample:\", error_normal)\n",
        "print(\"Reconstruction error on anomalous sample:\", error_anomalous)"
      ],
      "metadata": {
        "id": "-OZ9KlBI2kID",
        "outputId": "f6910a04-beba-444d-ef76-0e5af4735c17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 306ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Reconstruction error on normal sample: 0.41946134\n",
            "Reconstruction error on anomalous sample: 0.35970485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph autoencoder using a Formal Approach\n",
        "https://www.youtube.com/watch?v=qA6U4nIK62E"
      ],
      "metadata": {
        "id": "ZPQnRhUe60zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "BS43IjzS7kJ1",
        "outputId": "78e269b4-e7d2-4611-c29b-76f293acfcce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os.path as osp\n",
        "import time\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.nn import GAE, VGAE, GCNConv\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--variational', action='store_true')\n",
        "parser.add_argument('--linear', action='store_true')\n",
        "parser.add_argument('--dataset', type=str, default='Cora',\n",
        "                    choices=['Cora', 'CiteSeer', 'PubMed'])\n",
        "parser.add_argument('--epochs', type=int, default=400)\n",
        "parser.add_argument('--file', '-f', type=str)\n",
        "args = parser.parse_args()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.NormalizeFeatures(),\n",
        "    T.ToDevice(device),\n",
        "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
        "                      split_labels=True, add_negative_train_samples=False),\n",
        "])\n",
        "# path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'Planetoid')\n",
        "dataset = Planetoid('./sample_data', args.dataset, transform=transform)\n",
        "train_data, val_data, test_data = dataset[0]"
      ],
      "metadata": {
        "id": "EawHx1dk7QF5",
        "outputId": "d78cb4cb-db6f-4b28-e4f6-db66f28b3e4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
        "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "\n",
        "class VariationalGCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
        "        self.conv_mu = GCNConv(2 * out_channels, out_channels)\n",
        "        self.conv_logstd = GCNConv(2 * out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
        "\n",
        "\n",
        "class LinearEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = GCNConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        return self.conv(x, edge_index)\n",
        "\n",
        "\n",
        "class VariationalLinearEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_mu = GCNConv(in_channels, out_channels)\n",
        "        self.conv_logstd = GCNConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)"
      ],
      "metadata": {
        "id": "hAMUQKJ07-km"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels, out_channels = dataset.num_features, 16\n",
        "\n",
        "if not args.variational and not args.linear:\n",
        "    model = GAE(GCNEncoder(in_channels, out_channels))\n",
        "elif not args.variational and args.linear:\n",
        "    model = GAE(LinearEncoder(in_channels, out_channels))\n",
        "elif args.variational and not args.linear:\n",
        "    model = VGAE(VariationalGCNEncoder(in_channels, out_channels))\n",
        "elif args.variational and args.linear:\n",
        "    model = VGAE(VariationalLinearEncoder(in_channels, out_channels))\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(train_data.x, train_data.edge_index)\n",
        "    loss = model.recon_loss(z, train_data.pos_edge_label_index)\n",
        "    if args.variational:\n",
        "        loss = loss + (1 / train_data.num_nodes) * model.kl_loss()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(data):\n",
        "    model.eval()\n",
        "    z = model.encode(data.x, data.edge_index)\n",
        "    return model.test(z, data.pos_edge_label_index, data.neg_edge_label_index)\n",
        "\n",
        "\n",
        "times = []\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    start = time.time()\n",
        "    loss = train()\n",
        "    auc, ap = test(test_data)\n",
        "    print(f'Epoch: {epoch:03d}, AUC: {auc:.4f}, AP: {ap:.4f}')\n",
        "    times.append(time.time() - start)\n",
        "print(f\"Median time per epoch: {torch.tensor(times).median():.4f}s\")"
      ],
      "metadata": {
        "id": "fLtH9kkt8CB0",
        "outputId": "7088e976-9c87-4b03-bd40-0c7352b52489",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, AUC: 0.6657, AP: 0.6997\n",
            "Epoch: 002, AUC: 0.6576, AP: 0.6955\n",
            "Epoch: 003, AUC: 0.6539, AP: 0.6937\n",
            "Epoch: 004, AUC: 0.6525, AP: 0.6935\n",
            "Epoch: 005, AUC: 0.6524, AP: 0.6954\n",
            "Epoch: 006, AUC: 0.6534, AP: 0.6980\n",
            "Epoch: 007, AUC: 0.6575, AP: 0.7032\n",
            "Epoch: 008, AUC: 0.6622, AP: 0.7076\n",
            "Epoch: 009, AUC: 0.6656, AP: 0.7106\n",
            "Epoch: 010, AUC: 0.6666, AP: 0.7116\n",
            "Epoch: 011, AUC: 0.6674, AP: 0.7124\n",
            "Epoch: 012, AUC: 0.6685, AP: 0.7135\n",
            "Epoch: 013, AUC: 0.6714, AP: 0.7156\n",
            "Epoch: 014, AUC: 0.6760, AP: 0.7175\n",
            "Epoch: 015, AUC: 0.6868, AP: 0.7226\n",
            "Epoch: 016, AUC: 0.7055, AP: 0.7303\n",
            "Epoch: 017, AUC: 0.7222, AP: 0.7385\n",
            "Epoch: 018, AUC: 0.7286, AP: 0.7421\n",
            "Epoch: 019, AUC: 0.7340, AP: 0.7453\n",
            "Epoch: 020, AUC: 0.7465, AP: 0.7533\n",
            "Epoch: 021, AUC: 0.7642, AP: 0.7672\n",
            "Epoch: 022, AUC: 0.7782, AP: 0.7811\n",
            "Epoch: 023, AUC: 0.7832, AP: 0.7862\n",
            "Epoch: 024, AUC: 0.7824, AP: 0.7858\n",
            "Epoch: 025, AUC: 0.7833, AP: 0.7859\n",
            "Epoch: 026, AUC: 0.7827, AP: 0.7855\n",
            "Epoch: 027, AUC: 0.7769, AP: 0.7807\n",
            "Epoch: 028, AUC: 0.7719, AP: 0.7741\n",
            "Epoch: 029, AUC: 0.7751, AP: 0.7781\n",
            "Epoch: 030, AUC: 0.7759, AP: 0.7787\n",
            "Epoch: 031, AUC: 0.7747, AP: 0.7770\n",
            "Epoch: 032, AUC: 0.7754, AP: 0.7772\n",
            "Epoch: 033, AUC: 0.7800, AP: 0.7826\n",
            "Epoch: 034, AUC: 0.7901, AP: 0.7952\n",
            "Epoch: 035, AUC: 0.7973, AP: 0.8034\n",
            "Epoch: 036, AUC: 0.8022, AP: 0.8088\n",
            "Epoch: 037, AUC: 0.8057, AP: 0.8127\n",
            "Epoch: 038, AUC: 0.8153, AP: 0.8224\n",
            "Epoch: 039, AUC: 0.8248, AP: 0.8311\n",
            "Epoch: 040, AUC: 0.8319, AP: 0.8373\n",
            "Epoch: 041, AUC: 0.8347, AP: 0.8406\n",
            "Epoch: 042, AUC: 0.8377, AP: 0.8435\n",
            "Epoch: 043, AUC: 0.8418, AP: 0.8471\n",
            "Epoch: 044, AUC: 0.8455, AP: 0.8503\n",
            "Epoch: 045, AUC: 0.8481, AP: 0.8522\n",
            "Epoch: 046, AUC: 0.8472, AP: 0.8507\n",
            "Epoch: 047, AUC: 0.8458, AP: 0.8485\n",
            "Epoch: 048, AUC: 0.8472, AP: 0.8496\n",
            "Epoch: 049, AUC: 0.8495, AP: 0.8522\n",
            "Epoch: 050, AUC: 0.8481, AP: 0.8496\n",
            "Epoch: 051, AUC: 0.8469, AP: 0.8484\n",
            "Epoch: 052, AUC: 0.8479, AP: 0.8503\n",
            "Epoch: 053, AUC: 0.8483, AP: 0.8513\n",
            "Epoch: 054, AUC: 0.8505, AP: 0.8538\n",
            "Epoch: 055, AUC: 0.8513, AP: 0.8543\n",
            "Epoch: 056, AUC: 0.8517, AP: 0.8544\n",
            "Epoch: 057, AUC: 0.8517, AP: 0.8552\n",
            "Epoch: 058, AUC: 0.8539, AP: 0.8594\n",
            "Epoch: 059, AUC: 0.8582, AP: 0.8654\n",
            "Epoch: 060, AUC: 0.8624, AP: 0.8700\n",
            "Epoch: 061, AUC: 0.8637, AP: 0.8716\n",
            "Epoch: 062, AUC: 0.8612, AP: 0.8691\n",
            "Epoch: 063, AUC: 0.8591, AP: 0.8663\n",
            "Epoch: 064, AUC: 0.8621, AP: 0.8692\n",
            "Epoch: 065, AUC: 0.8671, AP: 0.8737\n",
            "Epoch: 066, AUC: 0.8692, AP: 0.8760\n",
            "Epoch: 067, AUC: 0.8701, AP: 0.8769\n",
            "Epoch: 068, AUC: 0.8713, AP: 0.8774\n",
            "Epoch: 069, AUC: 0.8735, AP: 0.8787\n",
            "Epoch: 070, AUC: 0.8748, AP: 0.8795\n",
            "Epoch: 071, AUC: 0.8753, AP: 0.8797\n",
            "Epoch: 072, AUC: 0.8777, AP: 0.8817\n",
            "Epoch: 073, AUC: 0.8788, AP: 0.8822\n",
            "Epoch: 074, AUC: 0.8786, AP: 0.8814\n",
            "Epoch: 075, AUC: 0.8796, AP: 0.8820\n",
            "Epoch: 076, AUC: 0.8809, AP: 0.8829\n",
            "Epoch: 077, AUC: 0.8821, AP: 0.8838\n",
            "Epoch: 078, AUC: 0.8813, AP: 0.8827\n",
            "Epoch: 079, AUC: 0.8822, AP: 0.8831\n",
            "Epoch: 080, AUC: 0.8844, AP: 0.8849\n",
            "Epoch: 081, AUC: 0.8855, AP: 0.8854\n",
            "Epoch: 082, AUC: 0.8863, AP: 0.8866\n",
            "Epoch: 083, AUC: 0.8875, AP: 0.8883\n",
            "Epoch: 084, AUC: 0.8884, AP: 0.8893\n",
            "Epoch: 085, AUC: 0.8883, AP: 0.8886\n",
            "Epoch: 086, AUC: 0.8900, AP: 0.8896\n",
            "Epoch: 087, AUC: 0.8919, AP: 0.8916\n",
            "Epoch: 088, AUC: 0.8943, AP: 0.8939\n",
            "Epoch: 089, AUC: 0.8950, AP: 0.8950\n",
            "Epoch: 090, AUC: 0.8950, AP: 0.8949\n",
            "Epoch: 091, AUC: 0.8958, AP: 0.8954\n",
            "Epoch: 092, AUC: 0.8985, AP: 0.8978\n",
            "Epoch: 093, AUC: 0.9009, AP: 0.9002\n",
            "Epoch: 094, AUC: 0.9017, AP: 0.9009\n",
            "Epoch: 095, AUC: 0.8996, AP: 0.8991\n",
            "Epoch: 096, AUC: 0.8999, AP: 0.8990\n",
            "Epoch: 097, AUC: 0.9022, AP: 0.9008\n",
            "Epoch: 098, AUC: 0.9050, AP: 0.9031\n",
            "Epoch: 099, AUC: 0.9066, AP: 0.9045\n",
            "Epoch: 100, AUC: 0.9069, AP: 0.9051\n",
            "Epoch: 101, AUC: 0.9054, AP: 0.9040\n",
            "Epoch: 102, AUC: 0.9054, AP: 0.9036\n",
            "Epoch: 103, AUC: 0.9073, AP: 0.9049\n",
            "Epoch: 104, AUC: 0.9096, AP: 0.9067\n",
            "Epoch: 105, AUC: 0.9108, AP: 0.9077\n",
            "Epoch: 106, AUC: 0.9109, AP: 0.9082\n",
            "Epoch: 107, AUC: 0.9114, AP: 0.9082\n",
            "Epoch: 108, AUC: 0.9112, AP: 0.9076\n",
            "Epoch: 109, AUC: 0.9113, AP: 0.9074\n",
            "Epoch: 110, AUC: 0.9120, AP: 0.9079\n",
            "Epoch: 111, AUC: 0.9118, AP: 0.9075\n",
            "Epoch: 112, AUC: 0.9116, AP: 0.9074\n",
            "Epoch: 113, AUC: 0.9120, AP: 0.9080\n",
            "Epoch: 114, AUC: 0.9118, AP: 0.9077\n",
            "Epoch: 115, AUC: 0.9102, AP: 0.9062\n",
            "Epoch: 116, AUC: 0.9103, AP: 0.9061\n",
            "Epoch: 117, AUC: 0.9119, AP: 0.9074\n",
            "Epoch: 118, AUC: 0.9130, AP: 0.9086\n",
            "Epoch: 119, AUC: 0.9127, AP: 0.9085\n",
            "Epoch: 120, AUC: 0.9119, AP: 0.9077\n",
            "Epoch: 121, AUC: 0.9105, AP: 0.9064\n",
            "Epoch: 122, AUC: 0.9097, AP: 0.9060\n",
            "Epoch: 123, AUC: 0.9111, AP: 0.9075\n",
            "Epoch: 124, AUC: 0.9126, AP: 0.9091\n",
            "Epoch: 125, AUC: 0.9130, AP: 0.9093\n",
            "Epoch: 126, AUC: 0.9121, AP: 0.9081\n",
            "Epoch: 127, AUC: 0.9105, AP: 0.9065\n",
            "Epoch: 128, AUC: 0.9096, AP: 0.9060\n",
            "Epoch: 129, AUC: 0.9107, AP: 0.9072\n",
            "Epoch: 130, AUC: 0.9120, AP: 0.9085\n",
            "Epoch: 131, AUC: 0.9133, AP: 0.9098\n",
            "Epoch: 132, AUC: 0.9135, AP: 0.9096\n",
            "Epoch: 133, AUC: 0.9120, AP: 0.9082\n",
            "Epoch: 134, AUC: 0.9096, AP: 0.9061\n",
            "Epoch: 135, AUC: 0.9083, AP: 0.9053\n",
            "Epoch: 136, AUC: 0.9100, AP: 0.9071\n",
            "Epoch: 137, AUC: 0.9121, AP: 0.9091\n",
            "Epoch: 138, AUC: 0.9133, AP: 0.9101\n",
            "Epoch: 139, AUC: 0.9127, AP: 0.9094\n",
            "Epoch: 140, AUC: 0.9114, AP: 0.9083\n",
            "Epoch: 141, AUC: 0.9094, AP: 0.9061\n",
            "Epoch: 142, AUC: 0.9093, AP: 0.9059\n",
            "Epoch: 143, AUC: 0.9103, AP: 0.9071\n",
            "Epoch: 144, AUC: 0.9115, AP: 0.9087\n",
            "Epoch: 145, AUC: 0.9120, AP: 0.9092\n",
            "Epoch: 146, AUC: 0.9117, AP: 0.9089\n",
            "Epoch: 147, AUC: 0.9106, AP: 0.9077\n",
            "Epoch: 148, AUC: 0.9089, AP: 0.9058\n",
            "Epoch: 149, AUC: 0.9095, AP: 0.9066\n",
            "Epoch: 150, AUC: 0.9109, AP: 0.9086\n",
            "Epoch: 151, AUC: 0.9114, AP: 0.9094\n",
            "Epoch: 152, AUC: 0.9113, AP: 0.9093\n",
            "Epoch: 153, AUC: 0.9109, AP: 0.9090\n",
            "Epoch: 154, AUC: 0.9097, AP: 0.9076\n",
            "Epoch: 155, AUC: 0.9085, AP: 0.9064\n",
            "Epoch: 156, AUC: 0.9088, AP: 0.9070\n",
            "Epoch: 157, AUC: 0.9103, AP: 0.9088\n",
            "Epoch: 158, AUC: 0.9108, AP: 0.9092\n",
            "Epoch: 159, AUC: 0.9101, AP: 0.9084\n",
            "Epoch: 160, AUC: 0.9100, AP: 0.9082\n",
            "Epoch: 161, AUC: 0.9094, AP: 0.9080\n",
            "Epoch: 162, AUC: 0.9094, AP: 0.9082\n",
            "Epoch: 163, AUC: 0.9091, AP: 0.9080\n",
            "Epoch: 164, AUC: 0.9094, AP: 0.9083\n",
            "Epoch: 165, AUC: 0.9099, AP: 0.9090\n",
            "Epoch: 166, AUC: 0.9106, AP: 0.9098\n",
            "Epoch: 167, AUC: 0.9099, AP: 0.9092\n",
            "Epoch: 168, AUC: 0.9088, AP: 0.9082\n",
            "Epoch: 169, AUC: 0.9088, AP: 0.9078\n",
            "Epoch: 170, AUC: 0.9102, AP: 0.9091\n",
            "Epoch: 171, AUC: 0.9118, AP: 0.9106\n",
            "Epoch: 172, AUC: 0.9120, AP: 0.9108\n",
            "Epoch: 173, AUC: 0.9106, AP: 0.9094\n",
            "Epoch: 174, AUC: 0.9083, AP: 0.9069\n",
            "Epoch: 175, AUC: 0.9077, AP: 0.9061\n",
            "Epoch: 176, AUC: 0.9092, AP: 0.9079\n",
            "Epoch: 177, AUC: 0.9108, AP: 0.9100\n",
            "Epoch: 178, AUC: 0.9113, AP: 0.9101\n",
            "Epoch: 179, AUC: 0.9108, AP: 0.9093\n",
            "Epoch: 180, AUC: 0.9097, AP: 0.9074\n",
            "Epoch: 181, AUC: 0.9090, AP: 0.9068\n",
            "Epoch: 182, AUC: 0.9090, AP: 0.9073\n",
            "Epoch: 183, AUC: 0.9093, AP: 0.9083\n",
            "Epoch: 184, AUC: 0.9101, AP: 0.9092\n",
            "Epoch: 185, AUC: 0.9101, AP: 0.9083\n",
            "Epoch: 186, AUC: 0.9100, AP: 0.9075\n",
            "Epoch: 187, AUC: 0.9098, AP: 0.9075\n",
            "Epoch: 188, AUC: 0.9099, AP: 0.9086\n",
            "Epoch: 189, AUC: 0.9102, AP: 0.9093\n",
            "Epoch: 190, AUC: 0.9113, AP: 0.9101\n",
            "Epoch: 191, AUC: 0.9115, AP: 0.9095\n",
            "Epoch: 192, AUC: 0.9103, AP: 0.9077\n",
            "Epoch: 193, AUC: 0.9101, AP: 0.9076\n",
            "Epoch: 194, AUC: 0.9108, AP: 0.9090\n",
            "Epoch: 195, AUC: 0.9119, AP: 0.9108\n",
            "Epoch: 196, AUC: 0.9127, AP: 0.9117\n",
            "Epoch: 197, AUC: 0.9132, AP: 0.9120\n",
            "Epoch: 198, AUC: 0.9126, AP: 0.9106\n",
            "Epoch: 199, AUC: 0.9109, AP: 0.9085\n",
            "Epoch: 200, AUC: 0.9096, AP: 0.9070\n",
            "Epoch: 201, AUC: 0.9101, AP: 0.9080\n",
            "Epoch: 202, AUC: 0.9117, AP: 0.9105\n",
            "Epoch: 203, AUC: 0.9131, AP: 0.9117\n",
            "Epoch: 204, AUC: 0.9139, AP: 0.9122\n",
            "Epoch: 205, AUC: 0.9131, AP: 0.9110\n",
            "Epoch: 206, AUC: 0.9120, AP: 0.9100\n",
            "Epoch: 207, AUC: 0.9115, AP: 0.9096\n",
            "Epoch: 208, AUC: 0.9127, AP: 0.9107\n",
            "Epoch: 209, AUC: 0.9136, AP: 0.9118\n",
            "Epoch: 210, AUC: 0.9143, AP: 0.9125\n",
            "Epoch: 211, AUC: 0.9146, AP: 0.9126\n",
            "Epoch: 212, AUC: 0.9144, AP: 0.9127\n",
            "Epoch: 213, AUC: 0.9140, AP: 0.9123\n",
            "Epoch: 214, AUC: 0.9134, AP: 0.9112\n",
            "Epoch: 215, AUC: 0.9130, AP: 0.9109\n",
            "Epoch: 216, AUC: 0.9133, AP: 0.9112\n",
            "Epoch: 217, AUC: 0.9142, AP: 0.9124\n",
            "Epoch: 218, AUC: 0.9145, AP: 0.9130\n",
            "Epoch: 219, AUC: 0.9148, AP: 0.9129\n",
            "Epoch: 220, AUC: 0.9146, AP: 0.9122\n",
            "Epoch: 221, AUC: 0.9144, AP: 0.9119\n",
            "Epoch: 222, AUC: 0.9154, AP: 0.9127\n",
            "Epoch: 223, AUC: 0.9151, AP: 0.9127\n",
            "Epoch: 224, AUC: 0.9150, AP: 0.9128\n",
            "Epoch: 225, AUC: 0.9147, AP: 0.9122\n",
            "Epoch: 226, AUC: 0.9147, AP: 0.9119\n",
            "Epoch: 227, AUC: 0.9154, AP: 0.9126\n",
            "Epoch: 228, AUC: 0.9161, AP: 0.9129\n",
            "Epoch: 229, AUC: 0.9160, AP: 0.9132\n",
            "Epoch: 230, AUC: 0.9155, AP: 0.9127\n",
            "Epoch: 231, AUC: 0.9155, AP: 0.9126\n",
            "Epoch: 232, AUC: 0.9159, AP: 0.9125\n",
            "Epoch: 233, AUC: 0.9166, AP: 0.9132\n",
            "Epoch: 234, AUC: 0.9172, AP: 0.9144\n",
            "Epoch: 235, AUC: 0.9179, AP: 0.9154\n",
            "Epoch: 236, AUC: 0.9178, AP: 0.9152\n",
            "Epoch: 237, AUC: 0.9168, AP: 0.9140\n",
            "Epoch: 238, AUC: 0.9158, AP: 0.9127\n",
            "Epoch: 239, AUC: 0.9166, AP: 0.9139\n",
            "Epoch: 240, AUC: 0.9182, AP: 0.9159\n",
            "Epoch: 241, AUC: 0.9184, AP: 0.9160\n",
            "Epoch: 242, AUC: 0.9174, AP: 0.9145\n",
            "Epoch: 243, AUC: 0.9159, AP: 0.9126\n",
            "Epoch: 244, AUC: 0.9148, AP: 0.9115\n",
            "Epoch: 245, AUC: 0.9152, AP: 0.9128\n",
            "Epoch: 246, AUC: 0.9169, AP: 0.9145\n",
            "Epoch: 247, AUC: 0.9178, AP: 0.9152\n",
            "Epoch: 248, AUC: 0.9185, AP: 0.9156\n",
            "Epoch: 249, AUC: 0.9169, AP: 0.9140\n",
            "Epoch: 250, AUC: 0.9150, AP: 0.9120\n",
            "Epoch: 251, AUC: 0.9146, AP: 0.9117\n",
            "Epoch: 252, AUC: 0.9164, AP: 0.9136\n",
            "Epoch: 253, AUC: 0.9178, AP: 0.9148\n",
            "Epoch: 254, AUC: 0.9183, AP: 0.9149\n",
            "Epoch: 255, AUC: 0.9180, AP: 0.9146\n",
            "Epoch: 256, AUC: 0.9165, AP: 0.9134\n",
            "Epoch: 257, AUC: 0.9149, AP: 0.9118\n",
            "Epoch: 258, AUC: 0.9155, AP: 0.9122\n",
            "Epoch: 259, AUC: 0.9167, AP: 0.9130\n",
            "Epoch: 260, AUC: 0.9180, AP: 0.9141\n",
            "Epoch: 261, AUC: 0.9184, AP: 0.9148\n",
            "Epoch: 262, AUC: 0.9174, AP: 0.9141\n",
            "Epoch: 263, AUC: 0.9160, AP: 0.9128\n",
            "Epoch: 264, AUC: 0.9155, AP: 0.9123\n",
            "Epoch: 265, AUC: 0.9166, AP: 0.9133\n",
            "Epoch: 266, AUC: 0.9175, AP: 0.9141\n",
            "Epoch: 267, AUC: 0.9176, AP: 0.9145\n",
            "Epoch: 268, AUC: 0.9173, AP: 0.9143\n",
            "Epoch: 269, AUC: 0.9172, AP: 0.9137\n",
            "Epoch: 270, AUC: 0.9170, AP: 0.9132\n",
            "Epoch: 271, AUC: 0.9175, AP: 0.9137\n",
            "Epoch: 272, AUC: 0.9171, AP: 0.9138\n",
            "Epoch: 273, AUC: 0.9177, AP: 0.9145\n",
            "Epoch: 274, AUC: 0.9185, AP: 0.9156\n",
            "Epoch: 275, AUC: 0.9183, AP: 0.9156\n",
            "Epoch: 276, AUC: 0.9176, AP: 0.9145\n",
            "Epoch: 277, AUC: 0.9177, AP: 0.9146\n",
            "Epoch: 278, AUC: 0.9177, AP: 0.9144\n",
            "Epoch: 279, AUC: 0.9174, AP: 0.9145\n",
            "Epoch: 280, AUC: 0.9171, AP: 0.9148\n",
            "Epoch: 281, AUC: 0.9171, AP: 0.9154\n",
            "Epoch: 282, AUC: 0.9178, AP: 0.9155\n",
            "Epoch: 283, AUC: 0.9176, AP: 0.9148\n",
            "Epoch: 284, AUC: 0.9166, AP: 0.9135\n",
            "Epoch: 285, AUC: 0.9168, AP: 0.9139\n",
            "Epoch: 286, AUC: 0.9171, AP: 0.9147\n",
            "Epoch: 287, AUC: 0.9172, AP: 0.9154\n",
            "Epoch: 288, AUC: 0.9182, AP: 0.9160\n",
            "Epoch: 289, AUC: 0.9179, AP: 0.9154\n",
            "Epoch: 290, AUC: 0.9171, AP: 0.9145\n",
            "Epoch: 291, AUC: 0.9166, AP: 0.9140\n",
            "Epoch: 292, AUC: 0.9163, AP: 0.9140\n",
            "Epoch: 293, AUC: 0.9165, AP: 0.9144\n",
            "Epoch: 294, AUC: 0.9165, AP: 0.9146\n",
            "Epoch: 295, AUC: 0.9165, AP: 0.9142\n",
            "Epoch: 296, AUC: 0.9162, AP: 0.9140\n",
            "Epoch: 297, AUC: 0.9154, AP: 0.9130\n",
            "Epoch: 298, AUC: 0.9148, AP: 0.9124\n",
            "Epoch: 299, AUC: 0.9146, AP: 0.9122\n",
            "Epoch: 300, AUC: 0.9147, AP: 0.9125\n",
            "Epoch: 301, AUC: 0.9153, AP: 0.9134\n",
            "Epoch: 302, AUC: 0.9157, AP: 0.9138\n",
            "Epoch: 303, AUC: 0.9153, AP: 0.9135\n",
            "Epoch: 304, AUC: 0.9147, AP: 0.9128\n",
            "Epoch: 305, AUC: 0.9134, AP: 0.9118\n",
            "Epoch: 306, AUC: 0.9132, AP: 0.9118\n",
            "Epoch: 307, AUC: 0.9146, AP: 0.9130\n",
            "Epoch: 308, AUC: 0.9162, AP: 0.9144\n",
            "Epoch: 309, AUC: 0.9169, AP: 0.9153\n",
            "Epoch: 310, AUC: 0.9162, AP: 0.9152\n",
            "Epoch: 311, AUC: 0.9148, AP: 0.9138\n",
            "Epoch: 312, AUC: 0.9135, AP: 0.9121\n",
            "Epoch: 313, AUC: 0.9136, AP: 0.9118\n",
            "Epoch: 314, AUC: 0.9151, AP: 0.9129\n",
            "Epoch: 315, AUC: 0.9163, AP: 0.9144\n",
            "Epoch: 316, AUC: 0.9166, AP: 0.9154\n",
            "Epoch: 317, AUC: 0.9157, AP: 0.9151\n",
            "Epoch: 318, AUC: 0.9146, AP: 0.9140\n",
            "Epoch: 319, AUC: 0.9140, AP: 0.9126\n",
            "Epoch: 320, AUC: 0.9144, AP: 0.9124\n",
            "Epoch: 321, AUC: 0.9156, AP: 0.9140\n",
            "Epoch: 322, AUC: 0.9163, AP: 0.9155\n",
            "Epoch: 323, AUC: 0.9162, AP: 0.9160\n",
            "Epoch: 324, AUC: 0.9158, AP: 0.9160\n",
            "Epoch: 325, AUC: 0.9152, AP: 0.9148\n",
            "Epoch: 326, AUC: 0.9146, AP: 0.9141\n",
            "Epoch: 327, AUC: 0.9144, AP: 0.9137\n",
            "Epoch: 328, AUC: 0.9152, AP: 0.9154\n",
            "Epoch: 329, AUC: 0.9160, AP: 0.9169\n",
            "Epoch: 330, AUC: 0.9158, AP: 0.9169\n",
            "Epoch: 331, AUC: 0.9151, AP: 0.9164\n",
            "Epoch: 332, AUC: 0.9143, AP: 0.9153\n",
            "Epoch: 333, AUC: 0.9135, AP: 0.9141\n",
            "Epoch: 334, AUC: 0.9138, AP: 0.9144\n",
            "Epoch: 335, AUC: 0.9147, AP: 0.9157\n",
            "Epoch: 336, AUC: 0.9159, AP: 0.9170\n",
            "Epoch: 337, AUC: 0.9160, AP: 0.9173\n",
            "Epoch: 338, AUC: 0.9151, AP: 0.9170\n",
            "Epoch: 339, AUC: 0.9139, AP: 0.9160\n",
            "Epoch: 340, AUC: 0.9139, AP: 0.9159\n",
            "Epoch: 341, AUC: 0.9145, AP: 0.9163\n",
            "Epoch: 342, AUC: 0.9156, AP: 0.9166\n",
            "Epoch: 343, AUC: 0.9167, AP: 0.9176\n",
            "Epoch: 344, AUC: 0.9170, AP: 0.9184\n",
            "Epoch: 345, AUC: 0.9160, AP: 0.9182\n",
            "Epoch: 346, AUC: 0.9157, AP: 0.9182\n",
            "Epoch: 347, AUC: 0.9158, AP: 0.9184\n",
            "Epoch: 348, AUC: 0.9173, AP: 0.9193\n",
            "Epoch: 349, AUC: 0.9183, AP: 0.9196\n",
            "Epoch: 350, AUC: 0.9183, AP: 0.9195\n",
            "Epoch: 351, AUC: 0.9173, AP: 0.9193\n",
            "Epoch: 352, AUC: 0.9171, AP: 0.9198\n",
            "Epoch: 353, AUC: 0.9180, AP: 0.9208\n",
            "Epoch: 354, AUC: 0.9189, AP: 0.9212\n",
            "Epoch: 355, AUC: 0.9191, AP: 0.9207\n",
            "Epoch: 356, AUC: 0.9190, AP: 0.9210\n",
            "Epoch: 357, AUC: 0.9186, AP: 0.9210\n",
            "Epoch: 358, AUC: 0.9174, AP: 0.9201\n",
            "Epoch: 359, AUC: 0.9173, AP: 0.9198\n",
            "Epoch: 360, AUC: 0.9183, AP: 0.9203\n",
            "Epoch: 361, AUC: 0.9198, AP: 0.9218\n",
            "Epoch: 362, AUC: 0.9203, AP: 0.9227\n",
            "Epoch: 363, AUC: 0.9200, AP: 0.9230\n",
            "Epoch: 364, AUC: 0.9186, AP: 0.9214\n",
            "Epoch: 365, AUC: 0.9182, AP: 0.9207\n",
            "Epoch: 366, AUC: 0.9191, AP: 0.9211\n",
            "Epoch: 367, AUC: 0.9196, AP: 0.9214\n",
            "Epoch: 368, AUC: 0.9197, AP: 0.9219\n",
            "Epoch: 369, AUC: 0.9190, AP: 0.9218\n",
            "Epoch: 370, AUC: 0.9188, AP: 0.9215\n",
            "Epoch: 371, AUC: 0.9190, AP: 0.9212\n",
            "Epoch: 372, AUC: 0.9188, AP: 0.9205\n",
            "Epoch: 373, AUC: 0.9195, AP: 0.9211\n",
            "Epoch: 374, AUC: 0.9200, AP: 0.9222\n",
            "Epoch: 375, AUC: 0.9196, AP: 0.9218\n",
            "Epoch: 376, AUC: 0.9191, AP: 0.9214\n",
            "Epoch: 377, AUC: 0.9186, AP: 0.9212\n",
            "Epoch: 378, AUC: 0.9184, AP: 0.9212\n",
            "Epoch: 379, AUC: 0.9184, AP: 0.9209\n",
            "Epoch: 380, AUC: 0.9180, AP: 0.9208\n",
            "Epoch: 381, AUC: 0.9178, AP: 0.9205\n",
            "Epoch: 382, AUC: 0.9173, AP: 0.9200\n",
            "Epoch: 383, AUC: 0.9172, AP: 0.9199\n",
            "Epoch: 384, AUC: 0.9173, AP: 0.9198\n",
            "Epoch: 385, AUC: 0.9178, AP: 0.9204\n",
            "Epoch: 386, AUC: 0.9182, AP: 0.9209\n",
            "Epoch: 387, AUC: 0.9180, AP: 0.9212\n",
            "Epoch: 388, AUC: 0.9173, AP: 0.9207\n",
            "Epoch: 389, AUC: 0.9172, AP: 0.9202\n",
            "Epoch: 390, AUC: 0.9178, AP: 0.9199\n",
            "Epoch: 391, AUC: 0.9178, AP: 0.9198\n",
            "Epoch: 392, AUC: 0.9178, AP: 0.9201\n",
            "Epoch: 393, AUC: 0.9176, AP: 0.9206\n",
            "Epoch: 394, AUC: 0.9179, AP: 0.9210\n",
            "Epoch: 395, AUC: 0.9177, AP: 0.9207\n",
            "Epoch: 396, AUC: 0.9173, AP: 0.9197\n",
            "Epoch: 397, AUC: 0.9170, AP: 0.9192\n",
            "Epoch: 398, AUC: 0.9177, AP: 0.9201\n",
            "Epoch: 399, AUC: 0.9182, AP: 0.9207\n",
            "Epoch: 400, AUC: 0.9187, AP: 0.9213\n",
            "Median time per epoch: 0.0493s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph autoencoder using RGraph from PyOD library"
      ],
      "metadata": {
        "id": "ix2tT5Ll5EDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyod"
      ],
      "metadata": {
        "id": "YWyl8UiX5Jy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyod.models.vae import VAE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X1 = pd.read_csv(\"data/tsne_data/tsne_orizzontale/time_series_tsne1.csv\")\n",
        "\n",
        "X2 = pd.read_csv(\"data/tsne_data/tsne_orizzontale/time_series_tsne2.csv\")\n",
        "\n",
        "X_1 = X1.loc[:, X1.columns != 'Acquisition Number']\n",
        "X_2 = X2.loc[:, X2.columns != 'Acquisition Number']\n",
        "\n",
        "X_train = X_1.iloc[: , :1000]\n",
        "X_test = X_2.iloc[: , :1000]\n",
        "\n",
        "X_train = X_train.fillna(0)\n",
        "X_test = X_test.fillna(0)\n",
        "\n",
        "X_train = X_train.sample(frac = 1)\n",
        "X_test = X_test.sample(frac = 1)\n",
        "\n",
        "X_train, X_validate = train_test_split(X_train, test_size=0.2)\n",
        "\n",
        "X_train.shape, X_validate.shape, X_test.shape"
      ],
      "metadata": {
        "id": "5OKxajza5Stg",
        "outputId": "07c285c0-d2ed-463b-8e0c-6ff1229b1955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((169, 1000), (43, 1000), (198, 1000))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyod.models.lunar import LUNAR\n",
        "clf = LUNAR(\n",
        "    model_type = 'WEIGHT',\n",
        "    n_epochs = 70,\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "clf.fit(X_train)"
      ],
      "metadata": {
        "id": "PLrOqq3Z7D5F",
        "outputId": "22bcf87c-2f8e-4075-f5ba-40c739c42643",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 \t Train Score 0.263461 \t Val Score 0.716263\n",
            "Epoch 1 \t Train Score 0.585093 \t Val Score 0.712803\n",
            "Epoch 2 \t Train Score 0.615002 \t Val Score 0.712803\n",
            "Epoch 3 \t Train Score 0.622706 \t Val Score 0.712803\n",
            "Epoch 4 \t Train Score 0.665859 \t Val Score 0.712803\n",
            "Epoch 5 \t Train Score 0.691482 \t Val Score 0.716263\n",
            "Epoch 6 \t Train Score 0.698927 \t Val Score 0.716263\n",
            "Epoch 7 \t Train Score 0.70148 \t Val Score 0.716263\n",
            "Epoch 8 \t Train Score 0.702519 \t Val Score 0.716263\n",
            "Epoch 9 \t Train Score 0.702909 \t Val Score 0.716263\n",
            "Epoch 10 \t Train Score 0.703341 \t Val Score 0.716263\n",
            "Epoch 11 \t Train Score 0.703428 \t Val Score 0.716263\n",
            "Epoch 12 \t Train Score 0.703471 \t Val Score 0.716263\n",
            "Epoch 13 \t Train Score 0.703688 \t Val Score 0.716263\n",
            "Epoch 14 \t Train Score 0.703731 \t Val Score 0.716263\n",
            "Epoch 15 \t Train Score 0.703861 \t Val Score 0.716263\n",
            "Epoch 16 \t Train Score 0.703731 \t Val Score 0.716263\n",
            "Epoch 17 \t Train Score 0.704034 \t Val Score 0.716263\n",
            "Epoch 18 \t Train Score 0.704034 \t Val Score 0.719723\n",
            "Epoch 19 \t Train Score 0.705029 \t Val Score 0.719723\n",
            "Epoch 20 \t Train Score 0.707107 \t Val Score 0.716263\n",
            "Epoch 21 \t Train Score 0.711219 \t Val Score 0.719723\n",
            "Epoch 22 \t Train Score 0.71598 \t Val Score 0.723183\n",
            "Epoch 23 \t Train Score 0.719702 \t Val Score 0.719723\n",
            "Epoch 24 \t Train Score 0.722342 \t Val Score 0.719723\n",
            "Epoch 25 \t Train Score 0.723727 \t Val Score 0.723183\n",
            "Epoch 26 \t Train Score 0.724636 \t Val Score 0.723183\n",
            "Epoch 27 \t Train Score 0.725545 \t Val Score 0.719723\n",
            "Epoch 28 \t Train Score 0.726541 \t Val Score 0.719723\n",
            "Epoch 29 \t Train Score 0.728012 \t Val Score 0.723183\n",
            "Epoch 30 \t Train Score 0.730696 \t Val Score 0.719723\n",
            "Epoch 31 \t Train Score 0.733077 \t Val Score 0.719723\n",
            "Epoch 32 \t Train Score 0.735673 \t Val Score 0.716263\n",
            "Epoch 33 \t Train Score 0.736712 \t Val Score 0.716263\n",
            "Epoch 34 \t Train Score 0.736929 \t Val Score 0.716263\n",
            "Epoch 35 \t Train Score 0.736885 \t Val Score 0.716263\n",
            "Epoch 36 \t Train Score 0.737448 \t Val Score 0.716263\n",
            "Epoch 37 \t Train Score 0.738703 \t Val Score 0.716263\n",
            "Epoch 38 \t Train Score 0.739915 \t Val Score 0.716263\n",
            "Epoch 39 \t Train Score 0.742252 \t Val Score 0.712803\n",
            "Epoch 40 \t Train Score 0.745369 \t Val Score 0.719723\n",
            "Epoch 41 \t Train Score 0.748615 \t Val Score 0.712803\n",
            "Epoch 42 \t Train Score 0.753592 \t Val Score 0.712803\n",
            "Epoch 43 \t Train Score 0.757748 \t Val Score 0.712803\n",
            "Epoch 44 \t Train Score 0.760042 \t Val Score 0.712803\n",
            "Epoch 45 \t Train Score 0.761297 \t Val Score 0.712803\n",
            "Epoch 46 \t Train Score 0.761859 \t Val Score 0.712803\n",
            "Epoch 47 \t Train Score 0.762033 \t Val Score 0.712803\n",
            "Epoch 48 \t Train Score 0.761686 \t Val Score 0.712803\n",
            "Epoch 49 \t Train Score 0.761686 \t Val Score 0.712803\n",
            "Epoch 50 \t Train Score 0.764456 \t Val Score 0.712803\n",
            "Epoch 51 \t Train Score 0.768785 \t Val Score 0.712803\n",
            "Epoch 52 \t Train Score 0.775537 \t Val Score 0.712803\n",
            "Epoch 53 \t Train Score 0.773286 \t Val Score 0.712803\n",
            "Epoch 54 \t Train Score 0.771122 \t Val Score 0.719723\n",
            "Epoch 55 \t Train Score 0.793629 \t Val Score 0.723183\n",
            "Epoch 56 \t Train Score 0.787742 \t Val Score 0.726644\n",
            "Epoch 57 \t Train Score 0.795793 \t Val Score 0.726644\n",
            "Epoch 58 \t Train Score 0.805099 \t Val Score 0.726644\n",
            "Epoch 59 \t Train Score 0.807176 \t Val Score 0.726644\n",
            "Epoch 60 \t Train Score 0.809297 \t Val Score 0.726644\n",
            "Epoch 61 \t Train Score 0.812543 \t Val Score 0.726644\n",
            "Epoch 62 \t Train Score 0.814534 \t Val Score 0.726644\n",
            "Epoch 63 \t Train Score 0.815227 \t Val Score 0.726644\n",
            "Epoch 64 \t Train Score 0.816309 \t Val Score 0.726644\n",
            "Epoch 65 \t Train Score 0.81778 \t Val Score 0.726644\n",
            "Epoch 66 \t Train Score 0.818213 \t Val Score 0.726644\n",
            "Epoch 67 \t Train Score 0.818776 \t Val Score 0.726644\n",
            "Epoch 68 \t Train Score 0.818776 \t Val Score 0.726644\n",
            "Epoch 69 \t Train Score 0.818819 \t Val Score 0.726644\n",
            "Finished training...\n",
            "Best Model: Epoch 69 \t Train Score 0.8188192520775622 \t Val Score 0.726643598615917\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LUNAR(contamination=0.1, epsilon=0.1, lr=0.001, model_type='WEIGHT',\n",
              "   n_epochs=70, n_neighbours=5, negative_sampling='MIXED', proportion=1.0,\n",
              "   scaler=MinMaxScaler(), val_size=0.1, verbose=1, wd=0.1)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation section"
      ],
      "metadata": {
        "id": "7Z3486M6Fnnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = clf.predict(X_test) # (0: inliers, 1: outliers)\n",
        "\n",
        "TN = np.count_nonzero(b == 1)\n",
        "FP = np.count_nonzero(b == 0)\n",
        "print(f\"---------CASO 2----------\")\n",
        "print(f\"Percentuale di time series anomale: {TN/len(b)*100}%\")\n",
        "\n",
        "c = clf.predict(X_validate)\n",
        "FN = np.count_nonzero(c == 1)\n",
        "TP = np.count_nonzero(c == 0)\n",
        "print(f\"---------CASO 1 (validate)----------\")\n",
        "print(f\"Percentuale di time series anomale: {np.count_nonzero(c == 1)/len(c)*100}%\")"
      ],
      "metadata": {
        "id": "PLO-Yibd8vXT",
        "outputId": "91d0f2d9-4ed3-4802-9f22-02846fc8d685",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------CASO 2----------\n",
            "Percentuale di time series anomale: 46.96969696969697%\n",
            "---------CASO 1 (validate)----------\n",
            "Percentuale di time series anomale: 18.6046511627907%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = TP/(TP + FP)\n",
        "TPR = TP/(TP + FN)\n",
        "TNR = TN/(TN + FP)\n",
        "FPR = 1 - TNR\n",
        "F1 = (2*precision*TPR)/(precision+TPR)\n",
        "\n",
        "print(f\"Precision: {str(round(precision*100, 2))}%\")\n",
        "print(f\"Recall (True Positive Rate): {str(round(TPR*100, 2))}%\")\n",
        "print(f\"Specificity (True Negative Rate): {str(round(TNR*100, 2))}%\")\n",
        "print(f\"FPR (False Positive Rate): {str(round(FPR*100, 2))}%\")\n",
        "print(f\"F1 score: {str(round(F1*100, 2))}%\")"
      ],
      "metadata": {
        "id": "EObn0GJhAj4Q",
        "outputId": "f8da1e94-2557-4001-d991-9698e95c091e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 25.0%\n",
            "Recall (True Positive Rate): 81.4%\n",
            "Specificity (True Negative Rate): 46.97%\n",
            "FPR (False Positive Rate): 53.03%\n",
            "F1 score: 38.25%\n"
          ]
        }
      ]
    }
  ]
}
